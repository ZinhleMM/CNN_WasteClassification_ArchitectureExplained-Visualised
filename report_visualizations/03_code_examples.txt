
DETAILED CODE IMPLEMENTATION EXAMPLES
====================================

1. IMAGE PREPROCESSING
   ──────────────────

```python
def preprocess_aerosol_image(img_path: str) -> np.ndarray:
    """
    Preprocess aerosol container image for CNN input.

    Steps:
    1. Load image using PIL/OpenCV
    2. Resize to 224×224 (MobileNetV2 requirement)
    3. Convert to numpy array
    4. Normalize pixel values to [0,1]
    5. Add batch dimension
    """
    from PIL import Image
    import numpy as np

    # Load image
    image = Image.open(img_path).convert('RGB')
    print(f"Original size: {image.size}")

    # Resize to model input size
    image = image.resize((224, 224), Image.LANCZOS)
    print(f"Resized to: {image.size}")

    # Convert to numpy array
    img_array = np.array(image, dtype=np.float32)
    print(f"Array shape: {img_array.shape}")

    # Normalize pixel values [0,255] → [0,1]
    img_array = img_array / 255.0
    print(f"Normalized range: [{img_array.min():.3f}, {img_array.max():.3f}]")

    # Add batch dimension: (224,224,3) → (1,224,224,3)
    img_array = np.expand_dims(img_array, axis=0)
    print(f"Final shape: {img_array.shape}")

    return img_array

# Example output for aerosol image:
# Original size: (300, 400)
# Resized to: (224, 224)
# Array shape: (224, 224, 3)
# Normalized range: [0.000, 1.000]
# Final shape: (1, 224, 224, 3)
```

2. MODEL ARCHITECTURE DEFINITION
   ─────────────────────────────

```python
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, Model

def create_waste_classifier(num_classes=5):
    """
    Create CNN model for waste classification using transfer learning.

    Architecture:
    - MobileNetV2 as feature extractor (frozen)
    - Global average pooling for dimension reduction
    - Dense layers for classification
    - Dropout for regularization
    """

    # Load pre-trained MobileNetV2
    base_model = MobileNetV2(
        weights='imagenet',        # Pre-trained on ImageNet
        include_top=False,         # Exclude final classification layer
        input_shape=(224, 224, 3), # RGB input
        alpha=1.0,                 # Width multiplier
        dropout=0.001              # Dropout rate in base model
    )

    # Freeze base model weights
    base_model.trainable = False
    print(f"Base model parameters: {base_model.count_params():,}")

    # Add custom classification head
    inputs = tf.keras.Input(shape=(224, 224, 3))

    # Feature extraction
    x = base_model(inputs, training=False)
    print(f"Base model output shape: {x.shape}")

    # Global average pooling: (7,7,1280) → (1280,)
    x = layers.GlobalAveragePooling2D()(x)
    print(f"After pooling: {x.shape}")

    # Regularization
    x = layers.Dropout(0.2)(x)

    # Feature combination layer
    x = layers.Dense(128, activation='relu', name='feature_dense')(x)
    print(f"Dense layer output: {x.shape}")

    # Final regularization
    x = layers.Dropout(0.2)(x)

    # Classification layer
    predictions = layers.Dense(
        num_classes,
        activation='softmax',
        name='classification'
    )(x)
    print(f"Final output: {predictions.shape}")

    # Create model
    model = Model(inputs, predictions)

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'top_2_accuracy']
    )

    return model

# Model summary output:
# Base model parameters: 2,257,984
# Base model output shape: (None, 7, 7, 1280)
# After pooling: (None, 1280)
# Dense layer output: (None, 128)
# Final output: (None, 5)
```

3. FORWARD PASS THROUGH LAYERS
   ───────────────────────────

```python
def analyze_forward_pass(model, preprocessed_image):
    """
    Analyze forward pass through each layer with actual activations.
    """

    # Create intermediate models for layer analysis
    layer_outputs = []
    layer_names = []

    # Key layers to analyze
    target_layers = [
        'input_1',                    # Input
        'mobilenetv2_1.00_224',      # MobileNetV2 output
        'global_average_pooling2d',   # Pooling
        'feature_dense',              # Dense layer
        'classification'              # Final output
    ]

    for layer_name in target_layers:
        try:
            layer = model.get_layer(layer_name)
            intermediate_model = Model(inputs=model.input, outputs=layer.output)
            output = intermediate_model.predict(preprocessed_image, verbose=0)
            layer_outputs.append(output)
            layer_names.append(layer_name)

            print(f"Layer: {layer_name}")
            print(f"  Shape: {output.shape}")
            print(f"  Value range: [{output.min():.4f}, {output.max():.4f}]")
            print(f"  Mean activation: {output.mean():.4f}")
            print()

        except ValueError as e:
            print(f"Skipping layer {layer_name}: {e}")

    return layer_outputs, layer_names

# Example output for aerosol image:
# Layer: input_1
#   Shape: (1, 224, 224, 3)
#   Value range: [0.0000, 1.0000]
#   Mean activation: 0.4892
#
# Layer: mobilenetv2_1.00_224
#   Shape: (1, 7, 7, 1280)
#   Value range: [0.0000, 4.7832]
#   Mean activation: 0.3421
#
# Layer: global_average_pooling2d
#   Shape: (1, 1280)
#   Value range: [0.0000, 2.1456]
#   Mean activation: 0.3421
#
# Layer: feature_dense
#   Shape: (1, 128)
#   Value range: [0.0000, 1.8934]
#   Mean activation: 0.2876
#
# Layer: classification
#   Shape: (1, 5)
#   Value range: [0.0030, 0.8730]
#   Mean activation: 0.2000
```

4. CLASSIFICATION AND CONFIDENCE ANALYSIS
   ──────────────────────────────────────

```python
def analyze_classification_result(predictions, class_names):
    """
    Detailed analysis of classification results for aerosol containers.
    """

    # Extract probabilities
    probs = predictions[0]  # Remove batch dimension

    # Calculate confidence metrics
    top1_prob = max(probs)
    top1_class = class_names[list(probs).index(top1_prob)]

    sorted_results = sorted(zip(class_names, probs), key=lambda x: x[1], reverse=True)

    print("CLASSIFICATION ANALYSIS:")
    print("=" * 40)
    print(f"Top prediction: {top1_class} ({top1_prob:.1%})")
    print()

    print("All predictions:")
    for i, (class_name, prob) in enumerate(sorted_results):
        confidence_bar = "█" * int(prob * 50) + "░" * (50 - int(prob * 50))
        print(f"{i+1}. {class_name:8s}: {prob:.1%} |{confidence_bar[:20]}|")

    # Confidence analysis
    print()
    print("CONFIDENCE METRICS:")
    print("-" * 20)

    confidence_gap = sorted_results[0][1] - sorted_results[1][1]
    print(f"Confidence gap: {confidence_gap:.1%}")

    # Calculate entropy (uncertainty measure)
    entropy = -sum(p * (p > 0 and log(p) or 0) for p in probs)
    max_entropy = log(len(class_names))
    normalized_entropy = entropy / max_entropy

    print(f"Entropy: {entropy:.3f} (max: {max_entropy:.3f})")
    print(f"Normalized entropy: {normalized_entropy:.3f}")
    print(f"Uncertainty: {1 - top1_prob:.1%}")

    # Decision quality assessment
    if confidence_gap > 0.5:
        quality = "Very High"
    elif confidence_gap > 0.3:
        quality = "High"
    elif confidence_gap > 0.1:
        quality = "Medium"
    else:
        quality = "Low"

    print(f"Decision quality: {quality}")

    return {
        'predicted_class': top1_class,
        'confidence': top1_prob,
        'confidence_gap': confidence_gap,
        'entropy': entropy,
        'uncertainty': 1 - top1_prob,
        'quality': quality
    }

# Example output for aerosol containers:
# CLASSIFICATION ANALYSIS:
# ========================================
# Top prediction: metal (87.3%)
#
# All predictions:
# 1. metal    : 87.3% |██████████████████░░|
# 2. plastic  : 8.2%  |███░░░░░░░░░░░░░░░░░|
# 3. glass    : 2.8%  |█░░░░░░░░░░░░░░░░░░░|
# 4. organic  : 1.4%  |░░░░░░░░░░░░░░░░░░░░|
# 5. paper    : 0.3%  |░░░░░░░░░░░░░░░░░░░░|
#
# CONFIDENCE METRICS:
# --------------------
# Confidence gap: 79.1%
# Entropy: 0.456 (max: 1.609)
# Normalized entropy: 0.283
# Uncertainty: 12.7%
# Decision quality: Very High
```

5. ENVIRONMENTAL IMPACT CALCULATION
   ─────────────────────────────────

```python
def calculate_environmental_impact(classification_result, weight_kg=0.1):
    """
    Calculate environmental impact of proper waste classification.

    Uses EPA WARM model methodology for impact assessment.
    """

    # Emission factors for different materials (kg CO2e per kg material)
    emission_factors = {
        'metal': {
            'co2e_per_kg': -1.47,      # Negative = emissions avoided
            'water_l_per_kg': 25.3,
            'energy_kwh_per_kg': 8.7,
            'recycling_rate': 0.74,
            'material_value_per_kg': 0.89  # USD
        },
        'plastic': {
            'co2e_per_kg': -1.12,
            'water_l_per_kg': 18.2,
            'energy_kwh_per_kg': 6.1,
            'recycling_rate': 0.09,
            'material_value_per_kg': 0.23
        },
        'glass': {
            'co2e_per_kg': -0.52,
            'water_l_per_kg': 12.1,
            'energy_kwh_per_kg': 2.8,
            'recycling_rate': 0.27,
            'material_value_per_kg': 0.12
        },
        'paper': {
            'co2e_per_kg': -3.89,
            'water_l_per_kg': 35.7,
            'energy_kwh_per_kg': 4.2,
            'recycling_rate': 0.68,
            'material_value_per_kg': 0.18
        },
        'organic': {
            'co2e_per_kg': -0.89,
            'water_l_per_kg': 8.4,
            'energy_kwh_per_kg': 1.2,
            'recycling_rate': 0.45,  # Composting rate
            'material_value_per_kg': 0.05
        }
    }

    predicted_class = classification_result['predicted_class']
    confidence = classification_result['confidence']

    if predicted_class not in emission_factors:
        return None

    factors = emission_factors[predicted_class]

    # Calculate impacts
    co2_saved = abs(weight_kg * factors['co2e_per_kg'])
    water_saved = weight_kg * factors['water_l_per_kg']
    energy_saved = weight_kg * factors['energy_kwh_per_kg']
    material_value = weight_kg * factors['material_value_per_kg']

    # Adjust for classification confidence
    confidence_adjusted_co2 = co2_saved * confidence
    confidence_adjusted_value = material_value * confidence

    impact_analysis = {
        'material': predicted_class,
        'weight_kg': weight_kg,
        'classification_confidence': confidence,

        # Direct impacts
        'co2_saved_kg': co2_saved,
        'water_saved_litres': water_saved,
        'energy_saved_kwh': energy_saved,
        'material_value_usd': material_value,

        # Confidence-adjusted impacts
        'expected_co2_saved': confidence_adjusted_co2,
        'expected_value': confidence_adjusted_value,

        # Scaling factors
        'recycling_success_rate': factors['recycling_rate'],
        'total_environmental_benefit': co2_saved * factors['recycling_rate'],
        'economic_viability': material_value > 0.10  # Profitable threshold
    }

    return impact_analysis

# Example calculation for 100g aerosol container:
# {
#     'material': 'metal',
#     'weight_kg': 0.1,
#     'classification_confidence': 0.873,
#     'co2_saved_kg': 0.147,
#     'water_saved_litres': 2.53,
#     'energy_saved_kwh': 0.87,
#     'material_value_usd': 0.089,
#     'expected_co2_saved': 0.128,      # Confidence-adjusted
#     'expected_value': 0.078,          # Confidence-adjusted
#     'recycling_success_rate': 0.74,
#     'total_environmental_benefit': 0.109,
#     'economic_viability': False       # Below $0.10 threshold
# }
```

USAGE EXAMPLE - COMPLETE PIPELINE:
==================================

```python
# Complete analysis pipeline for aerosol container image
def complete_analysis_pipeline(image_path):
    """Run complete CNN analysis pipeline."""

    print("🔍 STARTING CNN ANALYSIS PIPELINE")
    print("=" * 50)

    # Step 1: Preprocess image
    print("Step 1: Preprocessing...")
    preprocessed_img = preprocess_aerosol_image(image_path)

    # Step 2: Load model
    print("Step 2: Loading model...")
    model = create_waste_classifier()

    # Step 3: Forward pass analysis
    print("Step 3: Forward pass analysis...")
    layer_outputs, layer_names = analyze_forward_pass(model, preprocessed_img)

    # Step 4: Classification
    print("Step 4: Classification...")
    predictions = model.predict(preprocessed_img, verbose=0)
    class_names = ['plastic', 'organic', 'metal', 'glass', 'paper']

    classification_result = analyze_classification_result(predictions, class_names)

    # Step 5: Environmental impact
    print("Step 5: Environmental impact...")
    impact = calculate_environmental_impact(classification_result, weight_kg=0.1)

    # Summary report
    print()
    print("📊 ANALYSIS SUMMARY:")
    print("-" * 20)
    print(f"Classification: {classification_result['predicted_class']} ({classification_result['confidence']:.1%})")
    print(f"Decision quality: {classification_result['quality']}")
    print(f"CO₂ saved: {impact['co2_saved_kg']:.3f} kg")
    print(f"Economic value: ${impact['material_value_usd']:.3f}")
    print(f"Recycling potential: {impact['recycling_success_rate']:.1%}")

    return {
        'classification': classification_result,
        'environmental_impact': impact,
        'layer_analysis': layer_outputs
    }

# Run analysis
results = complete_analysis_pipeline("uploads/sample_image.png")
```
